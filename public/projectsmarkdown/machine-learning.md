
Fokus var at gÃ¥ fra klassiske modeller til dybere arkitekturer og afslutte med NLP og embeddings.

---

## ğŸ“Œ Klassiske ML-modeller
- LineÃ¦r regression og logistisk regression
- BeslutningstrÃ¦er og SVM
- Fokus pÃ¥ **datasÃ¦tforstÃ¥else** og **prÃ¦processering**
- MÃ¥lemetrikker: accuracy, precision/recall, confusion matrix
- Erfaring med **bias/variance trade-off**

---

## ğŸ“Œ Generalisering & Computer Vision
- **CNN pÃ¥ MNIST**: Convolutional Neural Network til hÃ¥ndskriftgenkendelse
  - Arkitektur: Conv2D â†’ ReLU â†’ MaxPool â†’ Dropout â†’ Dense
  - Evaluering: confusion matrix og lÃ¦ringskurver
- **Overfitting & regularisering**
  - Kapacitetskontrol (antal parametre)
  - Early stopping, dropout og L2-regularisering
- Indsigt i hvordan **modelkompleksitet pÃ¥virker generalisering**

---

## ğŸ“Œ Natural Language Processing
- **Duplicate Question Detection (Quora dataset)**
  - Klassificering af `question1`, `question2` â†’ `is_duplicate`
  - **spaCy** til tokenisering og embeddings
  - **SVM** som klassifikator
  - Visualisering af embeddings med **t-SNE**
- Eksperimenter med egen pipeline for tekstreprÃ¦sentation
- Resultat: robust baseline og bedre forstÃ¥else af semantiske reprÃ¦sentationer

---

## ğŸ”§ Teknologistak
- **Python** (NumPy, pandas, matplotlib)
- **scikit-learn** (klassiske ML-modeller, SVM)
- **TensorFlow/Keras** (CNN)
- **spaCy** (NLP pipeline)
- **t-SNE** (visualisering af embeddings)

---

## ğŸ¯ LÃ¦ringsudbytte
- ForstÃ¥else for forskellen mellem simple ML-modeller og dybe netvÃ¦rk
- Praktisk erfaring med at hÃ¥ndtere **overfitting/generalisation**
- Indsigt i NLP-pipelines og embedding-rum
- Evne til at kombinere forskellige frameworks i en ML-workflow

---
DM mig i kontakt fanen, for Source kode eller rapportter vedr. forskellige modeller eller eventuel samarbejde. 
