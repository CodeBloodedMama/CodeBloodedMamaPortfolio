
Fokus var at gå fra klassiske modeller til dybere arkitekturer og afslutte med NLP og embeddings.

---

## 📌 Klassiske ML-modeller
- Lineær regression og logistisk regression
- Beslutningstræer og SVM
- Fokus på **datasætforståelse** og **præprocessering**
- Målemetrikker: accuracy, precision/recall, confusion matrix
- Erfaring med **bias/variance trade-off**

---

## 📌 Generalisering & Computer Vision
- **CNN på MNIST**: Convolutional Neural Network til håndskriftgenkendelse
  - Arkitektur: Conv2D → ReLU → MaxPool → Dropout → Dense
  - Evaluering: confusion matrix og læringskurver
- **Overfitting & regularisering**
  - Kapacitetskontrol (antal parametre)
  - Early stopping, dropout og L2-regularisering
- Indsigt i hvordan **modelkompleksitet påvirker generalisering**

---

## 📌 Natural Language Processing
- **Duplicate Question Detection (Quora dataset)**
  - Klassificering af `question1`, `question2` → `is_duplicate`
  - **spaCy** til tokenisering og embeddings
  - **SVM** som klassifikator
  - Visualisering af embeddings med **t-SNE**
- Eksperimenter med egen pipeline for tekstrepræsentation
- Resultat: robust baseline og bedre forståelse af semantiske repræsentationer

---

## 🔧 Teknologistak
- **Python** (NumPy, pandas, matplotlib)
- **scikit-learn** (klassiske ML-modeller, SVM)
- **TensorFlow/Keras** (CNN)
- **spaCy** (NLP pipeline)
- **t-SNE** (visualisering af embeddings)

---

## 🎯 Læringsudbytte
- Forståelse for forskellen mellem simple ML-modeller og dybe netværk
- Praktisk erfaring med at håndtere **overfitting/generalisation**
- Indsigt i NLP-pipelines og embedding-rum
- Evne til at kombinere forskellige frameworks i en ML-workflow

---
DM mig i kontakt fanen, for Source kode eller rapportter vedr. forskellige modeller eller eventuel samarbejde. 
